<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Weihao XUAN's Homepage</title>
    <meta name="author" content="Weihao Xuan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/web_icons/web.png" type="images/web_icons/web.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600;700&family=Source+Sans+Pro:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
      :root {
        --primary-color: #2c3e50;
        --secondary-color: #34495e;
        --accent-color: #3498db;
        --text-color: #2c3e50;
        --text-light: #7f8c8d;
        --text-muted: #95a5a6;
        --background-color: #ffffff;
        --section-bg: #f8f9fa;
        --border-color: #ecf0f1;
        --hover-color: #e8f4f8;
        --shadow: 0 2px 4px rgba(0,0,0,0.1);
        --shadow-hover: 0 4px 8px rgba(0,0,0,0.15);
        --light-accent: #ecf0f1;
        --subtle-bg: #fdfdfd;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family: 'Source Sans Pro', -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        line-height: 1.6;
        color: var(--text-color);
        background-color: var(--background-color);
        font-size: 16px;
      }

      .container {
        max-width: 1000px;
        margin: 0 auto;
        padding: 0 20px;
      }

      header {
        background: linear-gradient(135deg, #f8f9fa 0%, #ffffff 100%);
        border-bottom: 1px solid var(--border-color);
        padding: 50px 0;
        position: relative;
      }


      .profile-container {
        display: flex;
        align-items: center;
        gap: 40px;
        margin-top: 20px;
      }

      @media (max-width: 768px) {
        .profile-container {
          flex-direction: column;
          align-items: center;
          text-align: center;
          gap: 30px;
        }
      }

      .profile-image {
        width: 180px;
        height: 180px;
        border-radius: 50%;
        object-fit: cover;
        border: 3px solid var(--background-color);
        box-shadow: var(--shadow-hover);
        flex-shrink: 0;
        transition: all 0.3s ease;
      }

      .profile-image:hover {
        transform: scale(1.05);
        box-shadow: 0 8px 25px rgba(0,0,0,0.15);
      }

      .profile-content {
        flex: 1;
      }

      h1 {
        font-family: 'Source Serif Pro', 'Hiragino Sans GB', 'Microsoft YaHei', '微软雅黑', 'STHeiti', 'SimHei', serif;
        font-size: 2.2em;
        font-weight: 700;
        margin-bottom: 15px;
        color: var(--primary-color);
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
        text-rendering: optimizeLegibility;
        letter-spacing: 0.5px;
        line-height: 1.2;
        transform: translateZ(0);
        -webkit-backface-visibility: hidden;
        backface-visibility: hidden;
        -webkit-perspective: 1000;
        perspective: 1000;
      }

      .profile-content p {
        margin-bottom: 15px;
        line-height: 1.7;
      }

      .social-links {
        display: flex;
        gap: 15px;
        margin-top: 20px;
        flex-wrap: wrap;
      }

      @media (max-width: 768px) {
        .social-links {
          justify-content: center;
        }
      }

      .social-links a {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 8px 16px;
        background-color: var(--background-color);
        border: 1px solid var(--border-color);
        border-radius: 4px;
        color: var(--accent-color);
        text-decoration: none;
        font-weight: 500;
        transition: all 0.2s ease;
        font-size: 14px;
      }

      .social-links a:hover {
        background-color: var(--hover-color);
        border-color: var(--accent-color);
        box-shadow: var(--shadow);
      }

      nav {
        background-color: var(--background-color);
        border-bottom: 1px solid var(--border-color);
        padding: 15px 0;
        z-index: 100;
      }

      .nav-links {
        display: flex;
        justify-content: center;
        gap: 30px;
        list-style: none;
      }

      .nav-links a {
        color: var(--text-color);
        text-decoration: none;
        font-weight: 600;
        padding: 10px 0;
        border-bottom: 2px solid transparent;
        transition: all 0.2s ease;
      }

      .nav-links a:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
      }

      section {
        padding: 25px 0;
      }

      .section-title {
        font-family: 'Source Serif Pro', serif;
        font-size: 2em;
        font-weight: 700;
        margin-bottom: 40px;
        color: var(--primary-color);
        border-bottom: 3px solid var(--accent-color);
        padding-bottom: 10px;
        position: relative;
      }

      .section-title::after {
        content: '';
        position: absolute;
        bottom: -3px;
        left: 0;
        width: 60px;
        height: 3px;
        background: linear-gradient(90deg, var(--accent-color), transparent);
      }

      .pub-category-title {
        font-family: 'Source Serif Pro', serif;
        font-size: 1.4em;
        font-weight: 600;
        margin: 35px 0 25px 0;
        color: var(--secondary-color);
        border-bottom: 2px solid var(--border-color);
        padding-bottom: 8px;
      }

      .pub-list {
        list-style: none;
        padding: 0;
      }

      .pub-item {
        margin-bottom: 25px;
        padding: 20px;
        background: linear-gradient(135deg, var(--background-color) 0%, var(--subtle-bg) 100%);
        border: 1px solid var(--border-color);
        border-radius: 8px;
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
      }

      .pub-item::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 4px;
        height: 100%;
        background: var(--accent-color);
        opacity: 0;
        transition: opacity 0.3s ease;
      }

      .pub-item:hover {
        box-shadow: var(--shadow-hover);
        border-color: var(--accent-color);
        transform: translateY(-2px);
      }

      .pub-item:hover::before {
        opacity: 1;
      }

      .pub-authors {
        margin-bottom: 8px;
        font-weight: 500;
        color: var(--text-color);
        display: block;
      }

      .pub-authors strong {
        color: var(--primary-color);
      }

      .pub-title {
        font-weight: 700;
        color: var(--primary-color);
        margin-bottom: 8px;
        font-size: 1.05em;
        line-height: 1.4;
        display: block;
      }

      .pub-venue {
        font-style: italic;
        color: var(--text-light);
        margin-bottom: 8px;
        display: block;
      }

      .pub-note {
        font-size: 0.9em;
        color: var(--text-muted);
        margin-bottom: 8px;
        display: block;
      }

      .pub-cofirst-note {
        color: var(--text-muted);
        font-size: 0.9em;
      }

      .pub-badge {
        display: block;
        background-color: var(--accent-color);
        color: white;
        padding: 3px 8px;
        border-radius: 3px;
        font-size: 0.8em;
        font-weight: 600;
        margin-bottom: 8px;
        margin-right: 8px;
        width: fit-content;
      }

      .pub-links {
        display: flex;
        gap: 10px;
        margin-top: 10px;
        flex-wrap: wrap;
      }

      .pub-links a {
        display: inline-flex;
        align-items: center;
        gap: 4px;
        padding: 5px 10px;
        background-color: var(--section-bg);
        border: 1px solid var(--border-color);
        color: var(--accent-color);
        text-decoration: none;
        border-radius: 3px;
        font-size: 0.85em;
        font-weight: 500;
        transition: all 0.2s ease;
      }

      .pub-links a:hover {
        background-color: var(--accent-color);
        color: white;
        border-color: var(--accent-color);
      }

      .activities-content {
        /* No special styling - matches education section */
      }

      .activities-content h3 {
        color: var(--primary-color);
        margin-bottom: 15px;
        font-size: 1.3em;
        font-weight: 600;
      }

      .activities-content p {
        line-height: 1.7;
        margin-bottom: 15px;
      }

      .activities-content strong {
        color: var(--primary-color);
      }

      .funding-content p {
        margin-bottom: 15px;
        line-height: 1.7;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }

      .funding-content strong {
        color: var(--primary-color);
      }

      .funding-date {
        color: var(--text-light);
        font-size: 0.9em;
        flex-shrink: 0;
        margin-left: 20px;
      }

      .news-content {
        /* No special styling - matches activities section */
      }

      .news-content p {
        line-height: 1.7;
        margin-bottom: 15px;
      }

      .news-content strong {
        color: var(--primary-color);
      }

      .education-content p {
        margin-bottom: 15px;
        line-height: 1.7;
      }

      .education-content strong {
        color: var(--primary-color);
      }

      footer {
        background-color: var(--section-bg);
        color: var(--text-muted);
        text-align: center;
        padding: 30px 0;
        margin-top: 50px;
        border-top: 1px solid var(--border-color);
      }

      footer p {
        margin: 0;
        font-size: 0.9em;
      }

      a {
        color: var(--accent-color);
        text-decoration: none;
        transition: color 0.2s ease;
      }

      a:hover {
        color: var(--primary-color);
        text-decoration: underline;
      }

      #about p {
        margin-bottom: 15px;
        line-height: 1.7;
      }

      #about b {
        color: var(--primary-color);
        font-weight: 600;
      }

      .back-to-top {
        position: fixed;
        bottom: 30px;
        right: 30px;
        background-color: var(--accent-color);
        color: white;
        width: 50px;
        height: 50px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        text-decoration: none;
        opacity: 0;
        transition: all 0.3s ease;
        box-shadow: var(--shadow);
        z-index: 1000;
      }

      .back-to-top.visible {
        opacity: 1;
      }

      .back-to-top:hover {
        background-color: var(--primary-color);
        transform: translateY(-2px);
        box-shadow: var(--shadow-hover);
      }

      @media (max-width: 768px) {
        .container {
          padding: 0 15px;
        }

        h1 {
          font-size: 1.8em;
        }

        .section-title {
          font-size: 1.6em;
        }

        .nav-links {
          gap: 20px;
        }

        .nav-links a {
          font-size: 0.9em;
        }

        section {
          padding: 20px 0;
        }

        .pub-item {
          padding: 15px;
        }

        .activities-content {
          /* No special mobile styling needed */
        }
      }
    </style>
  </head>

  <body>
    <a href="#top" class="back-to-top" id="backToTop">
      <i class="fas fa-arrow-up"></i>
    </a>

    <header id="top">
      <div class="container">
        <div class="profile-container">
          <img class="profile-image" src="images/personal/Weihao.jpg" alt="Weihao XUAN">
          <div class="profile-content">
            <h1>Weihao XUAN (宣 偉豪)</h1>
            <p>
              I'm a Ph.D. candidate at <a href="https://www.ms.k.u-tokyo.ac.jp/members.html">Machine Learning and Statistical Data Analysis Lab (杉山・横矢・石田研究室)</a>, <a href="https://www.u-tokyo.ac.jp/">The University of Tokyo (東京大学)</a>, where I'm very fortunate to be advised by Prof. <a href="https://naotoyokoya.com/">Naoto Yokoya</a>. I'm also under the Junior Research Associate (JRA) program at <a href="https://www.riken.jp/">RIKEN</a> <a href="https://aip.riken.jp/?lang=en">Center for Advanced Intelligence Project</a>.
            </p>

            <div id="about">
              <p>
                My main research focuses on <b>natural language understanding</b>, particularly in evaluation and post-training. I'm also actively engaged in <b>AI for Social Good</b>, applying NLP techniques and foundation models to Earth Observation and Medical domains through collaborative research.
                I collaborate very closely with my friends <a href="https://scholar.google.co.jp/citations?user=CH-rTXsAAAAJ&hl=en">Heli Qi</a> and <a href="https://scholar.google.com.hk/citations?user=H58gKSAAAAAJ&hl=en">Junjue Wang</a>, as well as brilliant researchers in LLM from the United States, Singapore, and Japan.
              </p>
            </div>

            <div class="social-links">
              <a href="mailto:weihaoxuanfoo@gmail.com"><i class="fas fa-envelope"></i> Email</a>
              <a href="https://github.com/weihao1115"><i class="fab fa-github"></i> Github</a>
              <a href="https://scholar.google.com/citations?hl=en&user=7e0W-2AAAAAJ"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
            </div>
          </div>
        </div>
      </div>
    </header>

    <nav>
      <div class="container">
        <ul class="nav-links">
          <li><a href="#news">News</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#education">Education</a></li>
          <li><a href="#activities">Activities</a></li>
          <li><a href="#funding">Funding & Awards</a></li>
        </ul>
      </div>
    </nav>

    <section id="news">
      <div class="container">
        <h2 class="section-title">News</h2>
        <div class="news-content">
          <p><strong>[08/2025]</strong> Three papers were accepted by EMNLP 2025 Main Conference, one selected as Oral Paper (Full Meta Score 10/10).</p>
          <p><strong>[06/2025]</strong> One paper was accepted by IROS 2025.</p>
          <p><strong>[09/2024]</strong> One paper was accepted by NeurIPS 2024 and selected as Spotlight Paper.</p>
        </div>
      </div>
    </section>

    <section id="publications">
      <div class="container">
        <h2 class="section-title">Publications</h2>

        <div class="publications-container">
          <div class="pub-category">

            <h3 class="pub-category-title">Preprints</h3>
            <ol class="pub-list">

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">The Invisible Leash: Why RLVR May Not Escape Its Origin</span>
                  <span class="pub-authors">Fang Wu*, <strong>Weihao Xuan*</strong>, Ximing Lu, Zaid Harchaoui, & Yejin Choi† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">arXiv preprint arXiv:2507.14843 (also in ICML 2025 AI4MATH Workshop).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2507.14843" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>


              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding</span>
                  <span class="pub-authors"><strong>Weihao Xuan*</strong>, Junjue Wang*, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, & Naoto Yokoya† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">arXiv preprint arXiv:2505.21076.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.21076" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response</span>
                  <span class="pub-authors">Junjue Wang*, <strong>Weihao Xuan*</strong>, Heli Qi, Zhihao Liu, Kunyi Liu, Yuhan Wu, Hongruixuan Chen, Jian Song, Junshi Xia, Zhuo Zheng, & Naoto Yokoya† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">arXiv preprint arXiv:2505.21089.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.21089" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>


              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models</span>
                  <span class="pub-authors">Kefan Yu, Qingcheng Zeng, <strong>Weihao Xuan</strong>, Wanxin Li, Jingyi Wu, & Rob Voigt†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2505.18497 (also in COLM 2025 PragLM Workshop).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.18497" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">VeriGUI: Verifiable Long-Chain GUI Dataset</span>
                  <span class="pub-authors">Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, <strong>Weihao Xuan</strong>, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song†, & Dacheng Tao†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2508.04026.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2508.04026" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/2077AIDataFoundation/VeriGUI" target="_blank"><i class="fas fa-database"></i> Dataset</a>
                    <a href="https://github.com/VeriGUI-Team/VeriGUI" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents</span>
                  <span class="pub-authors">Junjie Wang†, Yuxiang Zhang, Minghao Liu, Yin Zhang, Yatai Ji, <strong>Weihao Xuan</strong>, Nie Lin, Kang Zhu, Zhiqiang Lin, Yiming Ren, Chunyang Jiang, Yiyao Yu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Qunshu Liu, Yujiu Yang, Ge Zhang, Ruibin Yuan†, Bei Chen†, & Wenhu Chen†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2406.13923.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2406.13923" target="_blank"><i class="fas fa-file-pdf"></i>Paper</a>
                    <a href="https://huggingface.co/datasets/m-a-p/PIN-14M" target="_blank"><i class="fas fa-database"></i> Dataset (14M)</a>
                    <a href="https://huggingface.co/datasets/m-a-p/PIN-200M" target="_blank"><i class="fas fa-database"></i> Dataset (200M)</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">BRIGHT: A Globally Distributed Multimodal Building Damage Assessment Dataset With Very-High-Resolution for All-Weather Disaster Response</span>
                  <span class="pub-authors">Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, <strong>Weihao Xuan</strong>, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, & Naoto Yokoya†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2501.06019 (also in ICCV 2025 SEA Workshop).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2501.06019" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/Kullervo/BRIGHT" target="_blank"><i class="fas fa-database"></i> Dataset</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Is Pre-Training Applicable to the Decoder for Dense Prediction</span>
                  <span class="pub-authors">Chao Ning, Wanshui Gan, <strong>Weihao Xuan</strong>, & Naoto Yokoya†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2503.07637.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2503.07637" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>


              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Segment Anything With Multiple Modalities</span>
                  <span class="pub-authors">Aoran Xiao*, <strong>Weihao Xuan*</strong>, Heli Qi, Yun Xing, Naoto Yokoya†, & Shijian Lu† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">arXiv preprint arXiv:2408.09085.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2408.09085" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/weihao1115/mm-sam" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>
            </ol>

            <h3 class="pub-category-title">Conference Papers</h3>
            <ol class="pub-list">
              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Seeing Is Believing, But How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models</span>
                  <span class="pub-authors"><strong>Weihao Xuan*</strong>, Qingcheng Zeng*, Heli Qi, Junjue Wang, & Naoto Yokoya† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <span class="pub-badge">Oral Paper (Full Meta Score 10/10)</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.20236" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation</span>
                  <span class="pub-authors"><strong>Weihao Xuan†</strong>, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing, Junjue Wang, Fan Gao, Jinghui Lu, Yuang Jiang, Huitao Li, Xin Li, Kunyu Yu, Ruihai Dong, Shangding Gu, Yuekang Li, Xiaofei Xie, Felix Juefei-Xu, Foutse Khomh, Osamu Yoshie, Qingyu Chen, Douglas Teodoro, Nan Liu, Randy Goebel, Lei Ma, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, & Irene Li†</span>
                  <span class="pub-venue">The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2503.10497" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/li-lab/MMLU-ProX" target="_blank"><i class="fas fa-database"></i> Dataset (Full)</a>
                    <a href="https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite" target="_blank"><i class="fas fa-database"></i> Dataset (Lite)</a>
                    <a href="https://mmluprox.github.io/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Thinking Out Loud: Do Reasoning Models Know When They're Right?</span>
                  <span class="pub-authors">Qingcheng Zeng*, <strong>Weihao Xuan*</strong>, Leyang Cui, & Rob Voigt† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2504.06564" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>
              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">LR2Depth: Large-Region Aggregation at Low Resolution for Efficient Monocular Depth Estimation</span>
                  <span class="pub-authors">Chao Ning, <strong>Weihao Xuan</strong>, Wanshui Gan, & Naoto Yokoya†</span>
                  <span class="pub-venue">In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</span>
                  <div class="pub-links">
                    <a href="#" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="#" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding From Monocular Remote Sensing Imagery</span>
                  <span class="pub-authors">Jian Song, Hongruixuan Chen, <strong>Weihao Xuan</strong>, Junshi Xia, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The Thirty-eight Conference on Neural Information Processing Systems (NeurIPS 2024).</span>
                  <span class="pub-badge">Spotlight Paper [Top 3.1%]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2406.18151" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/JTRNEO/SynRS3D" target="_blank"><i class="fas fa-database"></i> Dataset</a>
                    <a href="https://github.com/JTRNEO/SynRS3D" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Cat-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model</span>
                  <span class="pub-authors">Aoran Xiao*, <strong>Weihao Xuan*</strong>, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Ling Shao & Shijian Lu† <span class="pub-cofirst-note">(* indicates co-first authors)</span></span>
                  <span class="pub-venue">In European Conference on Computer Vision (ECCV 2024) (pp. 189-206).</span>
                  <span class="pub-badge">Oral Paper [Top 2.3%, 200/8585]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2402.03631" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/weihao1115/cat-sam" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds</span>
                  <span class="pub-authors">Aoran Xiao, Jiaxing Huang, <strong>Weihao Xuan</strong>, Ruijie Ren, Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik, Shijian Lu†, & Eric Xing</span>
                  <span class="pub-venue">In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023) (pp. 9382-9392).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2304.00690" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/xiaoaoran/SemanticSTF" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">MaskVO: Self-Supervised Visual Odometry With a Learnable Dynamic Mask</span>
                  <span class="pub-authors"><strong>Weihao Xuan</strong>, Ruijie Ren, Siyuan Wu, & Changhao Chen</span>
                  <span class="pub-venue">In 2022 IEEE/SICE International Symposium on System Integration (SII) (pp. 225-231). IEEE.</span>
                  <div class="pub-links">
                    <a href="#" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">On a Discrete-Time Network SIS Model With Opinion Dynamics</span>
                  <span class="pub-authors">Yixuan Lin, <strong>Weihao Xuan</strong>, Ruijie Ren, & Ji Liu†</span>
                  <span class="pub-venue">In 2021 60th IEEE Conference on Decision and Control (CDC) (pp. 2098-2103). IEEE.</span>
                  <div class="pub-links">
                    <a href="https://ieeexplore.ieee.org/abstract/document/9683347" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">On a Network SIS Model With Opinion Dynamics</span>
                  <span class="pub-authors"><strong>Weihao Xuan</strong>, Ruijie Ren, Philip E. Paré, Mengbin Ye, Sebastian Ruf, & Ji Liu†</span>
                  <span class="pub-venue">IFAC-PapersOnLine, 53(2), 2582-2587.</span>
                  <div class="pub-links">
                    <a href="https://www.sciencedirect.com/science/article/pii/S2405896320305863" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>
            </ol>

            <h3 class="pub-category-title">Journal Papers</h3>
            <ol class="pub-list">
              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Foundation Models for Remote Sensing and Earth Observation: A Survey</span>
                  <span class="pub-authors">Aoran Xiao, <strong>Weihao Xuan</strong>, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu†, & Naoto Yokoya†</span>
                  <span class="pub-venue">IEEE Geoscience and Remote Sensing Magazine.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2410.16602" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
      </div>
    </section>

    <section id="education">
      <div class="container">
        <h2 class="section-title">Education</h2>
        <div class="education-content">
          <p><strong>The University of Tokyo (東京大学)</strong><br>
          Ph.D. in Complexity Science and Engineering<br>
          <a href="https://www.ms.k.u-tokyo.ac.jp/members.html">Machine Learning and Statistical Data Analysis Lab (杉山・横矢・石田研究室)</a>, Advisor: Prof. <a href="https://naotoyokoya.com/">Naoto Yokoya</a><br>
          Junior Research Associate (JRA) program at <a href="https://www.riken.jp/">RIKEN</a> <a href="https://aip.riken.jp/?lang=en">Center for Advanced Intelligence Project</a></p>

          <p><strong>Waseda University (早稲田大学)</strong><br>
          M.Eng. in Computer Science<br>
          Okuma Memorial Scholarship (Top Student)</p>

          <p><strong>University of Leeds</strong><br>
          B.Eng. in Mechanical Engineering<br>
          First-Class Honours</p>
        </div>
      </div>
    </section>

    <section id="activities">
      <div class="container">
        <h2 class="section-title">Professional Activities</h2>
        <div class="activities-content">
          <h3>Reviewer</h3>
          <p>
            <strong>Conference:</strong> <em>NeurIPS</em>, <em>CVPR</em>, <em>ICCV</em>, <em>AAAI</em>, <em>ICCVW</em>, <em>ACMMM</em>, <em>IROS</em>, <em>ICDL</em>, <em>SII</em>, <em>CPHS</em><br>
            <strong>Journal:</strong> <em>Pattern Recognition</em>, <em>ISPRS Journal of Photogrammetry and Remote Sensing</em>, <em>IEEE Transactions on Geoscience and Remote Sensing</em>
          </p>

          <h3>Organization</h3>
          <p>
            <strong>Session Co-Chair:</strong> ER3: System Integration, <em>IEEE/SICE International Symposium on System Integration</em> (SII 2022)
          </p>

          <h3>Editorial Board</h3>
          <p>
            <strong>Guest Editor Assistant:</strong> Special Issue: Advancement of Multi-Source Remote Sensing Data Fusion in Environmental Monitoring, <em>Remote Sensing</em>
          </p>
        </div>
      </div>
    </section>

    <section id="funding">
      <div class="container">
        <h2 class="section-title">Funding and Awards</h2>
        <div class="funding-content">
<!--          <p><strong>JHPCN Joint Usage and Research Funding</strong><span class="funding-date">Mar 2025</span></p>-->

          <p><strong>NVIDIA Academic Grant</strong><span class="funding-date">Dec 2024</span></p>

          <p><strong>RIKEN Junior Research Associate</strong><span class="funding-date">Dec 2023</span></p>

          <p><strong>Okuma Memorial Scholarship (Top Student)</strong><span class="funding-date">Dec 2022</span></p>

          <p><strong>Monbukagakusho Honors Scholarship, JASSO</strong><span class="funding-date">Apr 2022</span></p>
        </div>
      </div>
    </section>

    <footer>
      <div class="container">
        <p>Weihao Xuan | Last updated: September 2025</p>
      </div>
    </footer>

    <script>
      // Back to top button functionality
      const backToTopButton = document.getElementById('backToTop');

      window.addEventListener('scroll', () => {
        if (window.scrollY > 300) {
          backToTopButton.classList.add('visible');
        } else {
          backToTopButton.classList.remove('visible');
        }
      });

      // Smooth scrolling for anchor links
      document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const target = document.querySelector(this.getAttribute('href'));
          if (target) {
            target.scrollIntoView({
              behavior: 'smooth',
              block: 'start'
            });
          }
        });
      });
    </script>
  </body>
</html>
