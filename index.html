<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Weihao XUAN's Homepage</title>
    <meta name="author" content="Weihao Xuan">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/web_icons/web.png" type="images/web_icons/web.png">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Source+Serif+Pro:wght@400;600;700&family=Source+Sans+Pro:wght@300;400;600;700&display=swap" rel="stylesheet">
    <style>
      :root {
        --primary-color: #5D4E37;
        --secondary-color: #7B6B5B;
        --accent-color: #8B4513;
        --text-color: #4A3F35;
        --text-light: #8B8178;
        --text-muted: #A89F96;
        --background-color: #FDFBF7;
        --section-bg: #F5F0E8;
        --border-color: #E8E0D5;
        --hover-color: #FFF8F0;
        --shadow: 0 1px 3px rgba(93,78,55,0.08);
        --shadow-hover: 0 4px 12px rgba(93,78,55,0.12);
        --light-accent: #EDE8E0;
        --subtle-bg: #FAF8F5;
        --highlight-bg: #FFF5E6;
      }

      * {
        box-sizing: border-box;
        margin: 0;
        padding: 0;
      }

      body {
        font-family: 'Source Sans Pro', -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
        line-height: 1.6;
        color: var(--text-color);
        background-color: var(--background-color);
        font-size: 16px;
      }

      .container {
        max-width: 1000px;
        margin: 0 auto;
        padding: 0 20px;
      }

      header {
        background: linear-gradient(135deg, #F5F0E8 0%, #FDFBF7 100%);
        border-bottom: 1px solid var(--border-color);
        padding: 50px 0;
        position: relative;
      }


      .profile-container {
        display: flex;
        align-items: flex-start;
        gap: 40px;
        margin-top: 20px;
      }

      @media (max-width: 768px) {
        .profile-container {
          flex-direction: column;
          align-items: center;
          text-align: center;
          gap: 30px;
        }

        .profile-image {
          margin-top: 0;
        }
      }

      .profile-image {
        width: 180px;
        height: 180px;
        border-radius: 50%;
        object-fit: cover;
        border: 3px solid var(--background-color);
        box-shadow: var(--shadow-hover);
        flex-shrink: 0;
        transition: all 0.3s ease;
        margin-top: 70px;
      }

      .profile-image:hover {
        transform: scale(1.05);
        box-shadow: 0 8px 25px rgba(0,0,0,0.15);
      }

      .profile-content {
        flex: 1;
      }

      h1 {
        font-family: 'Source Serif Pro', 'Hiragino Sans GB', 'Microsoft YaHei', '微软雅黑', 'STHeiti', 'SimHei', serif;
        font-size: 2.2em;
        font-weight: 700;
        margin-bottom: 15px;
        color: var(--primary-color);
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
        text-rendering: optimizeLegibility;
        letter-spacing: 0.5px;
        line-height: 1.2;
        transform: translateZ(0);
        -webkit-backface-visibility: hidden;
        backface-visibility: hidden;
        -webkit-perspective: 1000;
        perspective: 1000;
      }

      .profile-content p {
        margin-bottom: 15px;
        line-height: 1.7;
      }

      .social-links {
        display: flex;
        gap: 12px;
        margin-top: 20px;
        flex-wrap: wrap;
      }

      @media (max-width: 768px) {
        .social-links {
          justify-content: center;
        }
      }

      .social-links a {
        display: inline-flex;
        align-items: center;
        gap: 6px;
        padding: 8px 16px;
        background-color: var(--background-color);
        border: 1.5px solid var(--border-color);
        border-radius: 24px;
        color: var(--text-color);
        text-decoration: none;
        font-weight: 600;
        transition: all 0.3s ease;
        font-size: 14px;
        position: relative;
        overflow: hidden;
        box-shadow: 0 1px 3px rgba(0,0,0,0.05);
      }

      .social-links a::before {
        content: '';
        position: absolute;
        top: 0;
        left: -100%;
        width: 100%;
        height: 100%;
        background: linear-gradient(90deg, transparent, rgba(139, 69, 19, 0.05), transparent);
        transition: left 0.5s ease;
      }

      .social-links a:hover::before {
        left: 100%;
      }

      .social-links a:hover {
        background-color: var(--accent-color);
        border-color: var(--accent-color);
        color: white;
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(139, 69, 19, 0.25);
      }

      .social-links a i {
        font-size: 14px;
        opacity: 0.9;
        transition: all 0.2s ease;
      }

      .social-links a:hover i {
        opacity: 1;
      }

      nav {
        background-color: var(--background-color);
        border-bottom: 1px solid var(--border-color);
        padding: 15px 0;
        z-index: 100;
      }

      .nav-links {
        display: flex;
        justify-content: center;
        gap: 30px;
        list-style: none;
      }

      .nav-links a {
        color: var(--text-color);
        text-decoration: none;
        font-weight: 600;
        padding: 10px 0;
        border-bottom: 2px solid transparent;
        transition: all 0.2s ease;
      }

      .nav-links a:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
      }

      section {
        padding: 30px 0;
        position: relative;
      }

      section:not(:last-of-type)::after {
        content: '';
        position: absolute;
        bottom: 0;
        left: 50%;
        transform: translateX(-50%);
        width: 60px;
        height: 1px;
        background: linear-gradient(90deg, transparent, var(--border-color), transparent);
      }

      .section-title {
        font-family: 'Source Serif Pro', serif;
        font-size: 2em;
        font-weight: 700;
        margin-bottom: 40px;
        color: var(--primary-color);
        border-bottom: 3px solid var(--accent-color);
        padding-bottom: 10px;
        position: relative;
      }

      .section-title::after {
        content: '';
        position: absolute;
        bottom: -3px;
        left: 0;
        width: 60px;
        height: 3px;
        background: linear-gradient(90deg, var(--accent-color), transparent);
      }

      .pub-category-title {
        font-family: 'Source Serif Pro', serif;
        font-size: 1.4em;
        font-weight: 600;
        margin: 35px 0 25px 0;
        color: var(--secondary-color);
        border-bottom: 2px solid var(--border-color);
        padding-bottom: 8px;
      }

      .pub-list {
        list-style: none;
        padding: 0;
      }

      .pub-item {
        margin-bottom: 15px;
        padding: 16px;
        background: var(--background-color);
        border: 1px solid var(--border-color);
        border-radius: 8px;
        transition: all 0.25s ease;
        position: relative;
        overflow: hidden;
      }

      .pub-item:hover {
        border-color: var(--accent-color);
        box-shadow: var(--shadow-hover);
        background: var(--highlight-bg);
      }

      .pub-item::before {
        content: '';
        position: absolute;
        top: 0;
        left: 0;
        width: 4px;
        height: 100%;
        background: var(--accent-color);
        opacity: 0;
        transition: opacity 0.3s ease;
      }

      .pub-item:hover::before {
        opacity: 1;
      }

      .pub-item.featured {
        border: 1px solid rgba(139, 69, 19, 0.2);
        background: linear-gradient(135deg, var(--background-color) 0%, rgba(255, 245, 230, 0.3) 100%);
        box-shadow: 0 1px 4px rgba(139, 69, 19, 0.08);
      }

      .pub-item.featured:hover {
        border-color: rgba(139, 69, 19, 0.4);
        background: rgba(255, 245, 230, 0.5);
      }

      .pub-authors {
        margin-bottom: 5px;
        font-weight: 500;
        color: var(--text-color);
        display: block;
        font-size: 0.95em;
        line-height: 1.4;
      }

      .pub-authors strong {
        color: var(--primary-color);
      }

      .pub-title {
        font-weight: 700;
        color: var(--primary-color);
        margin-bottom: 5px;
        font-size: 1.02em;
        line-height: 1.3;
        display: block;
      }

      .pub-venue {
        font-style: italic;
        color: var(--text-light);
        margin-bottom: 5px;
        display: block;
        font-size: 0.92em;
        line-height: 1.3;
      }

      .pub-note {
        font-size: 0.85em;
        color: var(--text-muted);
        margin-bottom: 5px;
        display: block;
      }

      .pub-badge {
        display: inline-block;
        background: linear-gradient(135deg, #FF6B35, #FF8C42);
        color: white;
        padding: 3px 8px;
        border-radius: 12px;
        font-size: 0.75em;
        font-weight: 600;
        margin-bottom: 6px;
        margin-right: 8px;
        width: fit-content;
        box-shadow: 0 1px 3px rgba(255, 107, 53, 0.3);
      }

      .pub-badge-hf {
        display: inline-block;
        background: linear-gradient(135deg, #FF9D00, #FFB340);
        color: white;
        padding: 3px 8px;
        border-radius: 12px;
        font-size: 0.75em;
        font-weight: 600;
        margin-bottom: 6px;
        margin-right: 8px;
        width: fit-content;
        box-shadow: 0 1px 3px rgba(255, 157, 0, 0.3);
      }

      .pub-links {
        display: flex;
        gap: 6px;
        margin-top: 8px;
        flex-wrap: wrap;
      }

      .pub-links a {
        display: inline-flex;
        align-items: center;
        gap: 4px;
        padding: 4px 10px;
        background-color: var(--section-bg);
        border: 1px solid var(--border-color);
        color: var(--accent-color);
        text-decoration: none;
        border-radius: 6px;
        font-size: 0.8em;
        font-weight: 500;
        transition: all 0.2s ease;
      }

      .pub-links a:hover {
        background-color: var(--accent-color);
        color: white;
        border-color: var(--accent-color);
        transform: translateY(-1px);
        box-shadow: 0 2px 4px rgba(139, 69, 19, 0.2);
      }

      .github-stats, .hf-stats {
        display: inline-block;
        color: var(--accent-color);
        font-size: 0.9em;
        font-weight: 500;
        margin-left: 2px;
        white-space: nowrap;
        vertical-align: middle;
        transition: all 0.2s ease;
      }

      .pub-links a:hover .github-stats,
      .pub-links a:hover .hf-stats {
        color: rgba(255, 255, 255, 0.9);
      }

      .activities-content {
        max-width: 800px;
      }

      .activity-section {
        margin-bottom: 20px;
        padding-bottom: 15px;
        border-bottom: 1px solid var(--border-color);
      }

      .activity-section:last-child {
        border-bottom: none;
        margin-bottom: 0;
        padding-bottom: 0;
      }

      .activities-content h3 {
        color: var(--primary-color);
        margin-bottom: 12px;
        font-size: 1.15em;
        font-weight: 600;
        position: relative;
        padding-left: 12px;
      }

      .activities-content h3::before {
        content: '';
        position: absolute;
        left: 0;
        top: 2px;
        width: 3px;
        height: 20px;
        background: var(--accent-color);
        border-radius: 2px;
      }

      .activity-row {
        display: grid;
        grid-template-columns: 140px 1fr;
        gap: 15px;
        margin-bottom: 10px;
        align-items: baseline;
      }

      .activity-row:last-child {
        margin-bottom: 0;
      }

      .activity-label {
        color: var(--text-light);
        font-weight: 500;
        font-size: 0.9em;
        text-align: right;
        white-space: nowrap;
      }

      .activity-content {
        line-height: 1.5;
      }

      .activity-content em {
        color: var(--text-light);
        font-style: italic;
      }

      .activity-list {
        display: flex;
        flex-wrap: wrap;
        gap: 8px 16px;
        line-height: 1.4;
      }

      .activity-list em {
        white-space: nowrap;
      }

      @media (max-width: 768px) {
        .activity-row {
          grid-template-columns: 1fr;
          gap: 8px;
        }

        .activity-label {
          text-align: left;
          font-weight: 600;
          color: var(--primary-color);
        }
      }

      .funding-content {
        max-width: 800px;
        position: relative;
      }

      .funding-item {
        margin-bottom: 30px;
        padding-left: 30px;
        position: relative;
      }

      .funding-item:last-child {
        margin-bottom: 0;
      }

      .funding-item::before {
        content: '';
        position: absolute;
        left: 8px;
        top: 8px;
        width: 8px;
        height: 8px;
        background: var(--accent-color);
        border-radius: 50%;
        border: 2px solid var(--background-color);
        box-shadow: 0 0 0 2px var(--accent-color);
      }

      .funding-item::after {
        content: '';
        position: absolute;
        left: 11px;
        top: 20px;
        width: 2px;
        height: calc(100% + 10px);
        background: var(--border-color);
      }

      .funding-item:last-child::after {
        display: none;
      }

      .funding-title {
        color: var(--primary-color);
        font-weight: 600;
        font-size: 1.02em;
        margin-bottom: 6px;
        display: block;
      }

      .funding-date {
        color: var(--text-light);
        font-size: 0.9em;
        line-height: 1.6;
      }

      .news-content {
        /* No special styling - matches activities section */
      }

      .news-content p {
        line-height: 1.7;
        margin-bottom: 15px;
      }

      .news-content strong {
        color: var(--primary-color);
      }

      .news-content strong a {
        color: inherit;
        text-decoration: none;
        border-bottom: 1px solid transparent;
        transition: all 0.2s ease;
      }

      .news-content strong a i {
        transition: all 0.2s ease;
      }

      .news-content strong a:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
        text-decoration: none;
      }

      .news-content strong a:hover i {
        opacity: 1;
        transform: translateX(2px);
      }

      .news-toggle {
        display: inline-flex;
        align-items: center;
        gap: 4px;
        padding: 4px 0;
        background-color: transparent;
        border: none;
        border-bottom: 1px solid transparent;
        color: var(--text-muted);
        text-decoration: none;
        font-weight: 400;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 13px;
        margin-top: 20px;
        position: relative;
      }

      .news-toggle:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
      }

      .news-toggle i {
        font-size: 11px;
        transition: transform 0.3s ease;
      }

      .news-collapsible {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease;
      }

      .news-collapsible.expanded {
        max-height: 500px;
      }

      .education-content {
        max-width: 800px;
        position: relative;
      }

      .education-item {
        margin-bottom: 40px;
        padding-left: 30px;
        position: relative;
      }

      .education-item:last-child {
        margin-bottom: 0;
      }

      .education-item::before {
        content: '';
        position: absolute;
        left: 8px;
        top: 8px;
        width: 8px;
        height: 8px;
        background: var(--accent-color);
        border-radius: 50%;
        border: 2px solid var(--background-color);
        box-shadow: 0 0 0 2px var(--accent-color);
      }

      .education-item::after {
        content: '';
        position: absolute;
        left: 11px;
        top: 20px;
        width: 2px;
        height: calc(100% + 20px);
        background: var(--border-color);
      }

      .education-item:last-child::after {
        display: none;
      }

      .education-university {
        color: var(--primary-color);
        font-weight: 600;
        font-size: 1.02em;
        margin-bottom: 8px;
        display: block;
      }

      .education-degree {
        color: var(--text-color);
        font-weight: 500;
        margin-bottom: 6px;
        display: block;
      }

      .education-details {
        color: var(--text-light);
        line-height: 1.6;
        margin-bottom: 4px;
      }

      .education-details:last-child {
        margin-bottom: 0;
      }

      .education-highlight {
        color: var(--accent-color);
        font-weight: 600;
        position: relative;
      }

      .education-highlight::before {
        content: '•';
        color: var(--accent-color);
        margin-right: 4px;
        font-weight: bold;
      }

      .education-position {
        color: var(--text-color);
        font-style: italic;
      }

      .coauthors-content {
        max-width: 800px;
      }

      .coauthor-item {
        margin-bottom: 10px;
        line-height: 1.6;
        font-size: 0.98em;
        padding-left: 15px;
        position: relative;
      }

      #hiddenCoauthorsContainer .coauthor-item {
        margin-bottom: 8px;
        font-size: 0.92em;
        line-height: 1.5;
      }

      .coauthor-item:last-child {
        margin-bottom: 0;
      }

      .coauthor-item::before {
        content: '•';
        position: absolute;
        left: 0;
        top: 0;
        color: var(--accent-color);
        font-weight: bold;
      }

      .coauthor-item a {
        font-weight: 600;
        color: var(--primary-color);
        text-decoration: none;
        transition: all 0.2s ease;
      }

      .coauthor-item a:hover {
        color: var(--accent-color);
        text-decoration: underline;
      }

      .coauthor-item em {
        color: var(--text-light);
        font-style: italic;
      }

      .coauthors-collapsible {
        max-height: 0;
        overflow: hidden;
        transition: max-height 0.3s ease;
      }

      .coauthors-collapsible.expanded {
        max-height: 400px;
      }

      .coauthors-toggle {
        display: inline-flex;
        align-items: center;
        gap: 4px;
        padding: 4px 0;
        background-color: transparent;
        border: none;
        border-bottom: 1px solid transparent;
        color: var(--text-muted);
        text-decoration: none;
        font-weight: 400;
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 13px;
        margin-top: 20px;
        position: relative;
      }

      .coauthors-toggle:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
      }

      .coauthors-toggle i {
        font-size: 11px;
        transition: transform 0.3s ease;
      }

      footer {
        background-color: var(--section-bg);
        color: var(--text-muted);
        text-align: center;
        padding: 30px 0;
        margin-top: 50px;
        border-top: 1px solid var(--border-color);
      }

      footer p {
        margin: 0;
        font-size: 0.9em;
      }

      a {
        color: var(--accent-color);
        text-decoration: none;
        transition: color 0.2s ease;
      }

      a:hover {
        color: var(--primary-color);
        text-decoration: underline;
      }

      #about p {
        margin-bottom: 15px;
        line-height: 1.7;
      }

      #about b {
        color: var(--primary-color);
        font-weight: 600;
      }

      #collaboratorsLink:hover {
        color: var(--accent-color);
        border-bottom-color: var(--accent-color);
      }

      #collaboratorsLink:hover i {
        opacity: 1;
        transform: scale(1.1);
      }

      .back-to-top {
        position: fixed;
        bottom: 30px;
        right: 30px;
        background-color: var(--accent-color);
        color: white;
        width: 50px;
        height: 50px;
        border-radius: 50%;
        display: flex;
        align-items: center;
        justify-content: center;
        text-decoration: none;
        opacity: 0;
        transition: all 0.3s ease;
        box-shadow: var(--shadow);
        z-index: 1000;
      }

      .back-to-top.visible {
        opacity: 1;
      }

      .back-to-top:hover {
        background-color: var(--primary-color);
        transform: translateY(-2px);
        box-shadow: var(--shadow-hover);
      }

      @media (max-width: 768px) {
        .container {
          padding: 0 15px;
        }

        h1 {
          font-size: 1.8em;
        }

        .section-title {
          font-size: 1.6em;
        }

        .nav-links {
          gap: 20px;
        }

        .nav-links a {
          font-size: 0.9em;
        }

        section {
          padding: 20px 0;
        }

        .pub-item {
          padding: 12px;
          margin-bottom: 12px;
        }

        .activities-content {
          /* No special mobile styling needed */
        }
      }
    </style>
  </head>

  <body>
    <a href="#top" class="back-to-top" id="backToTop">
      <i class="fas fa-arrow-up"></i>
    </a>

    <header id="top">
      <div class="container">
        <div class="profile-container">
          <img class="profile-image" src="images/personal/Weihao.jpeg" alt="Weihao XUAN">
          <div class="profile-content">
            <h1>Weihao XUAN (宣 偉豪)</h1>
            <p>
              I'm a Ph.D. candidate at <a href="https://www.ms.k.u-tokyo.ac.jp/members.html">Machine Learning and Statistical Data Analysis Lab (杉山・横矢・石田研究室)</a>, <a href="https://www.u-tokyo.ac.jp/">The University of Tokyo (東京大学)</a>, where I'm very fortunate to be advised by Prof. <a href="https://naotoyokoya.com/">Naoto Yokoya</a>. I'm also under the Junior Research Associate (JRA) program at <a href="https://www.riken.jp/">RIKEN</a> <a href="https://aip.riken.jp/?lang=en">Center for Advanced Intelligence Project</a>.
            </p>

            <div id="about">
              <p>
                My research focuses on <b>natural language understanding</b>, particularly in evaluation and post-training. I'm also actively engaged in <b>AI for Social Good</b> (Earth Observation, Biomedical applications) and <b>AI for Science</b> (Biology, Material Science), applying NLP techniques and foundation models through collaborative research.
                I work with <a href="#" id="collaboratorsLink" style="cursor: pointer; color: inherit; text-decoration: none; transition: all 0.2s ease; border-bottom: 1px solid transparent;">amazing collaborators <i class="fas fa-users" style="font-size: 0.8em; opacity: 0.6; margin-left: 2px;"></i></a>.
              </p>
              <p style="margin-top: 15px; padding: 12px 16px; background: var(--highlight-bg); border-left: 3px solid var(--accent-color); border-radius: 4px;">
                <i class="fas fa-envelope" style="margin-right: 8px; color: var(--accent-color);"></i>If you are interested in working with me or joining our lab, feel free to <a href="mailto:weihaoxuan@g.ecc.u-tokyo.ac.jp">reach out via email</a>.
              </p>
            </div>

            <!-- Hidden collaborators section -->
            <div id="hiddenCollaborators" style="display: none; margin-top: 20px; padding: 18px; background: var(--section-bg); border-radius: 8px; border: 1px solid var(--border-color); opacity: 0; transition: opacity 0.3s ease-in-out; font-size: 0.95em;">
              <div style="color: var(--text-light); margin-bottom: 14px; font-size: 0.9em; font-weight: 500; text-transform: uppercase; letter-spacing: 0.5px; opacity: 0.7;">Regular Collaborators</div>
              <div class="coauthors-content" id="hiddenCoauthorsContainer" style="line-height: 1.6;">
                <div class="coauthor-item">
                  <a href="https://scholar.google.co.jp/citations?user=CH-rTXsAAAAJ&hl=en">Heli Qi</a>, RIKEN AIP & Waseda University <em>(<strong>everything</strong>)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com.hk/citations?user=H58gKSAAAAAJ&hl=en">Junjue Wang</a>, The University of Tokyo <em>(remote sensing)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com/citations?user=8vHkc5YAAAAJ&hl=en">Fang Wu</a>, Stanford University <em>(natural language processing & AI for Science)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com/citations?user=i0K71KQAAAAJ&hl">Qingcheng Zeng</a>, Northwestern University <em>(natural language processing)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com/citations?user=95n7XTkAAAAJ&hl=en">Lorenzo Xiao</a>, Carnegie Mellon University <em>(natural language processing: persona)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com/citations?user=aCawmg0AAAAJ&hl=enn">Rui Yang</a>, Duke-NUS Medical School <em>(natural language processing: medical LLMs)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com.hk/citations?user=XOk4Cf0AAAAJ&hl=en">Hongruixuan Chen</a>, The University of Tokyo & RIKEN AIP <em>(remote sensing: disaster response)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com.hk/citations?user=3SNYa0EAAAAJ&hl=en">Zhihao Liu</a>, The University of Tokyo & RIKEN AIP <em>(computer graphics: plant modeling)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com.hk/citations?user=CgcMFJsAAAAJ&hl=en">Jian Song</a>, The University of Tokyo & RIKEN AIP <em>(remote sensing: 3D reconstruction)</em>
                </div>
                <div class="coauthor-item">
                  <a href="https://scholar.google.com/citations?user=CREpn_AAAAAJ&hl=en">Zhuo Zheng</a>, Stanford University <em>(remote sensing)</em>
                </div>
              </div>
            </div>

            <div class="social-links">
              <a href="mailto:weihaoxuan@g.ecc.u-tokyo.ac.jp"><i class="fas fa-envelope"></i> Email</a>
              <a href="https://github.com/weihao1115"><i class="fab fa-github"></i> Github</a>
              <a href="https://scholar.google.com/citations?hl=en&user=7e0W-2AAAAAJ"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
            </div>
          </div>
        </div>
      </div>
    </header>

    <nav>
      <div class="container">
        <ul class="nav-links">
          <li><a href="#news">News</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#education">Education</a></li>
          <li><a href="#activities">Activities</a></li>
          <li><a href="#funding">Funding & Awards</a></li>
        </ul>
      </div>
    </nav>

    <section id="news">
      <div class="container">
        <h2 class="section-title">News</h2>
        <div class="news-content" id="newsContainer">
          <p><strong>[01/2026]</strong> Two papers accepted to ICLR 2026.</p>
          <p><strong>[01/2026]</strong> Two papers accepted to EACL 2026 Main Conference.</p>
          <p><strong>[11/2025]</strong> One paper accepted to AAAI 2026.</p>
          <p><strong>[09/2025]</strong> Our team won <strong>first place (1/261)</strong> in the <a href="https://ai4eo.eu/portfolio/ai-for-earthquake-response/" target="_blank">AI for Earthquake Response</a>, a competition funded by the European Space Agency, etc.</p>
          <p><strong>[09/2025]</strong> Two papers accepted to NeurIPS 2025.</p>
          <p><strong>[08/2025]</strong> Three papers accepted to EMNLP 2025 Main Conference, with one as an <strong>Oral Paper</strong>.</p>
          <p><strong>[06/2025]</strong> One paper accepted to IROS 2025.</p>
          <p><strong>[09/2024]</strong> One paper accepted to NeurIPS 2024 as a <strong>Spotlight Paper</strong>.</p>
          <p><strong>[08/2024]</strong> One paper accepted to ECCV 2024 as an <strong>Oral Paper</strong>.</p>
          <p><strong>[02/2023]</strong> One paper accepted to CVPR 2023.</p>
          <div class="news-collapsible" id="newsCollapsible">
          </div>
          <button class="news-toggle" id="newsToggle" style="display: none;">
            <i class="fas fa-chevron-down" id="newsToggleIcon"></i>
            <span id="newsToggleText">Show More</span>
          </button>
        </div>
      </div>
    </section>

    <section id="publications">
      <div class="container">
        <h2 class="section-title">Publications <span style="font-size: 0.45em; font-weight: normal; color: var(--text-muted); font-style: italic; margin-left: 0.5em;">(* co-first | † corresponding)</span></h2>

        <div class="publications-container">
          <div class="pub-category">

            <h3 class="pub-category-title">Preprints</h3>
            <ol class="pub-list">

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">The Confidence Dichotomy: Analyzing and Mitigating Miscalibration in Tool-Use Agents</span>
                  <span class="pub-authors"><strong>Weihao Xuan</strong>*, Qingcheng Zeng*, Heli Qi, Yunze Xiao, Junjue Wang, & Naoto Yokoya†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2601.07264.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2601.07264" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Towards Valid Student Simulation with Large Language Models</span>
                  <span class="pub-authors">Zhihao Yuan*, Yunze Xiao*, Ming Li*, <strong>Weihao Xuan</strong>, Richard Jiarui Tong, Mona Diab, & Tom Mitchell†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2601.05473.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2601.05473" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Toward Global Large Language Models in Medicine</span>
                  <span class="pub-authors">Rui Yang, Huitao Li, <strong>Weihao Xuan</strong>†, Heli Qi, Xin Li, Kunyu Yu, Yingjian Chen, Rongrong Wang, Jacques Behmoaras, Tianxi Cai, Bibhas Chakraborty, Qingyu Chen, Lionel Tim-Ee Cheng, Marie-Louise Damwanza, Chido Dzinotyiwei, Aosong Feng, Chuan Hong, Yusuke Iwasawa, Yuhe Ke, Linah Kitala, Taehoon Ko, Jisan Lee, Irene Li, Jonathan Chong Kai Liew, Hongfang Liu, Lian Leng Low, Edison Marrese-Taylor, Yutaka Matsuo, Isheanesu Misi, Yilin Ning, Jasmine Chiat Ling Ong, Marcus Eng Hock Ong, Enrico Petretto, Hossein Rouhizadeh, Abiram Sandralegar, Oren Schreier, Iain Bee Huat Tan, Patrick Tan, Daniel Shu Wei Ting, Junjue Wang, Chunhua Weng, Matthew Yu Heng Wong, Fang Wu, Yunze Xiao, Xuhai Xu, Qingcheng Zeng, Zhuo Zheng, Yifan Peng†, Douglas Teodoro†, & Nan Liu†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2601.02186. (under review)</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2601.02186" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">TeamPath: Building MultiModal Pathology Experts with Reasoning AI Copilots</span>
                  <span class="pub-authors">Tianyu Liu*, <strong>Weihao Xuan*</strong>, Hao Wu, Peter Humphrey, Marcello DiStasio, Heli Qi, Rui Yang, Simeng Han, Tinglin Huang, Fang Wu, Nan Liu, Irene Li, Hua Xu, & Hongyu Zhao†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2511.17652. (under review)</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2511.17652" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations</span>
                  <span class="pub-authors">Rui Yang*, Matthew Yu Heng Wong*, Huitao Li*, Xin Li, Wentao Zhu, Jingchi Liao, Kunyu Yu, Jonathan Chong Kai Liew, <strong>Weihao Xuan</strong>, Yingjian Chen, Yuhe Ke, Jasmine Chiat Ling Ong, Douglas Teodoro, Chuan Hong, Daniel Shu Wei Ting, & Nan Liu†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2511.05901. (under review)</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2511.05901" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards</span>
                  <span class="pub-authors">Aaron Tu*, <strong>Weihao Xuan*</strong>, Heli Qi*, Xu Huang, Qingcheng Zeng, Shayan Talaei, Yijia Xiao, Peng Xia, Xiangru Tang, Yuchen Zhuang, Bing Hu, Hanqun Cao, Wenqi Shi, Tianang Leng, Rui Yang, Yingjian Chen, Ziqi Wang, Irene Li, Nan Liu, Huaxiu Yao, Li Erran Li, Ge Liu, Amin Saberi, Naoto Yokoya, Jure Leskovec, Yejin Choi, Fang Wu*†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2509.21882.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2509.21882" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">The Invisible Leash: Why RLVR May Not Escape Its Origin</span>
                  <span class="pub-authors">Fang Wu*, <strong>Weihao Xuan*</strong>, Ximing Lu, Zaid Harchaoui, & Yejin Choi†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2507.14843 (also in ICML 2025 AI4MATH Workshop).</span>
                  <span class="pub-badge-hf"><i class="fas fa-fire"></i> HuggingFace Daily Papers Top 3 [2025.07.22]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2507.14843" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>





              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">VeriWeb: Verifiable Long-Chain Web Benchmark for Agentic Information-Seeking</span>
                  <span class="pub-authors">Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Jialiang Gao, Heng Zhou, Yunhao Yang, Wendong Fan, Puzhen Zhang, Ge Zhang, Jiajun Shi, <strong>Weihao Xuan</strong>, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Junjie Wang, Aosong Feng, Jindi Lv, Sicong Jiang, Ziqi Ren, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao</span>
                  <span class="pub-venue">arXiv preprint arXiv:2508.04026.</span>
                  <span class="pub-badge-hf"><i class="fas fa-fire"></i> HuggingFace Daily Papers Top 2 [2025.08.07]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2508.04026" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/2077AIDataFoundation/VeriGUI" target="_blank"><i class="fas fa-database"></i> Dataset <span class="hf-stats" data-dataset="2077AIDataFoundation/VeriGUI"><i class="fas fa-download"></i> ...</span></a>
                    <a href="https://github.com/VeriGUI-Team/VeriGUI" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="VeriGUI-Team/VeriGUI">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal Documents</span>
                  <span class="pub-authors">Junjie Wang†, Yuxiang Zhang, Minghao Liu, Yin Zhang, Yatai Ji, <strong>Weihao Xuan</strong>, Nie Lin, Kang Zhu, Zhiqiang Lin, Yiming Ren, Chunyang Jiang, Yiyao Yu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Qunshu Liu, Yujiu Yang, Ge Zhang, Ruibin Yuan†, Bei Chen†, & Wenhu Chen†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2406.13923.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2406.13923" target="_blank"><i class="fas fa-file-pdf"></i>Paper</a>
                    <a href="https://huggingface.co/datasets/m-a-p/PIN-14M" target="_blank"><i class="fas fa-database"></i> Dataset (14M) <span class="hf-stats" data-dataset="m-a-p/PIN-14M"><i class="fas fa-download"></i> ...</span></a>
                    <a href="https://huggingface.co/datasets/m-a-p/PIN-200M" target="_blank"><i class="fas fa-database"></i> Dataset (200M) <span class="hf-stats" data-dataset="m-a-p/PIN-200M"><i class="fas fa-download"></i> ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Is Pre-Training Applicable to the Decoder for Dense Prediction</span>
                  <span class="pub-authors">Chao Ning, Wanshui Gan, <strong>Weihao Xuan</strong>, & Naoto Yokoya†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2503.07637.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2503.07637" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>


              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Segment Anything With Multiple Modalities</span>
                  <span class="pub-authors">Aoran Xiao*, <strong>Weihao Xuan*</strong>, Heli Qi, Yun Xing, Naoto Yokoya†, & Shijian Lu†</span>
                  <span class="pub-venue">arXiv preprint arXiv:2408.09085.</span>
                  <span class="pub-badge-hf"><i class="fas fa-fire"></i> HuggingFace Daily Papers Top 3 [2024.08.20]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2408.09085" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/weihao1115/mm-sam" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="weihao1115/mm-sam">★ ...</span></a>
                  </div>
                </div>
              </li>
            </ol>

            <h3 class="pub-category-title">Conference Papers</h3>
            <ol class="pub-list">
              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search</span>
                  <span class="pub-authors">Fang Wu*, <strong>Weihao Xuan*</strong>, Heli Qi*, Ximing Lu, Aaron Tu, Li Erran Li, Yejin Choi†</span>
                  <span class="pub-venue">In The Fourteenth International Conference on Learning Representations (ICLR 2026).</span>
                  <span class="pub-badge-hf"><i class="fas fa-fire"></i> HuggingFace Daily Papers Top 1 [2025.10.02]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2509.25454" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/fangwu97/DeepSearch-1.5B" target="_blank"><i class="fas fa-cube"></i> Model <span class="hf-stats" data-model="fangwu97/DeepSearch-1.5B"><i class="fas fa-download"></i> ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Multiplayer Nash Preference Optimization</span>
                  <span class="pub-authors">Fang Wu*, Xu Huang*, <strong>Weihao Xuan</strong>, Zhiwei Zhang, Yijia Xiao, Guancheng Wan, Xiaomin Li, Bing Hu, Peng Xia, Jure Leskovec, Yejin Choi†</span>
                  <span class="pub-venue">In The Fourteenth International Conference on Learning Representations (ICLR 2026).</span>
                  <span class="pub-badge-hf"><i class="fas fa-fire"></i> HuggingFace Daily Papers Top 3 [2025.09.30]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2509.23102" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/smiles724/MNPO" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="smiles724/MNPO">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Taming Object Hallucinations with Verified Atomic Confidence Estimation</span>
                  <span class="pub-authors">Jiarui Liu, <strong>Weihao Xuan</strong>, Zhijing Jin, Mona Diab†</span>
                  <span class="pub-venue">In The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2511.09228" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models</span>
                  <span class="pub-authors">Kefan Yu, Qingcheng Zeng, <strong>Weihao Xuan</strong>, Wanxin Li, Jingyi Wu, & Rob Voigt†</span>
                  <span class="pub-venue">In The 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026) (also in COLM 2025 PragLM Workshop).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.18497" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">LandCraft: Designing the Structured 3D Landscapes via Text Guidance</span>
                  <span class="pub-authors">Zhihao Liu*, Fang Liu*, <strong>Weihao Xuan</strong>, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026).</span>
                  <div class="pub-links">
                    <a href="#" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/RyuZhihao123/LandCraft_26" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="RyuZhihao123/LandCraft_26">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">DynamicVL: Benchmarking Multimodal Large Language Models for Dynamic City Understanding</span>
                  <span class="pub-authors"><strong>Weihao Xuan*</strong>, Junjue Wang*, Heli Qi, Zihang Chen, Zhuo Zheng, Yanfei Zhong, Junshi Xia, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.21076" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/weihao1115/dynamicvl" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="weihao1115/dynamicvl">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">DisasterM3: A Remote Sensing Vision-Language Dataset for Disaster Damage Assessment and Response</span>
                  <span class="pub-authors">Junjue Wang*, <strong>Weihao Xuan*</strong>, Heli Qi, Zhihao Liu, Kunyi Liu, Yuhan Wu, Hongruixuan Chen, Jian Song, Junshi Xia, Zhuo Zheng, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The Thirty-ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.21089" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/Junjue-Wang/DisasterM3" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="Junjue-Wang/DisasterM3">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item featured" id="emnlp2025-oral">
                <div class="pub-content">
                  <span class="pub-title">Seeing Is Believing, But How Much? A Comprehensive Analysis of Verbalized Calibration in Vision-Language Models</span>
                  <span class="pub-authors"><strong>Weihao Xuan*</strong>, Qingcheng Zeng*, Heli Qi, Junjue Wang, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <span class="pub-badge"><i class="fas fa-microphone"></i> Oral Paper [Full Meta Score 10/10]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2505.20236" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">MMLU-ProX: A Multilingual Benchmark for Advanced Large Language Model Evaluation</span>
                  <span class="pub-authors"><strong>Weihao Xuan†</strong>, Rui Yang, Heli Qi, Qingcheng Zeng, Yunze Xiao, Aosong Feng, Dairui Liu, Yun Xing, Junjue Wang, Fan Gao, Jinghui Lu, Yuang Jiang, Huitao Li, Xin Li, Kunyu Yu, Ruihai Dong, Shangding Gu, Yuekang Li, Xiaofei Xie, Felix Juefei-Xu, Foutse Khomh, Osamu Yoshie, Qingyu Chen, Douglas Teodoro, Nan Liu, Randy Goebel, Lei Ma, Edison Marrese-Taylor, Shijian Lu, Yusuke Iwasawa, Yutaka Matsuo, & Irene Li†</span>
                  <span class="pub-venue">In The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2503.10497" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/li-lab/MMLU-ProX" target="_blank"><i class="fas fa-database"></i> Dataset (Full) <span class="hf-stats" data-dataset="li-lab/MMLU-ProX"><i class="fas fa-download"></i> ...</span></a>
                    <a href="https://huggingface.co/datasets/li-lab/MMLU-ProX-Lite" target="_blank"><i class="fas fa-database"></i> Dataset (Lite) <span class="hf-stats" data-dataset="li-lab/MMLU-ProX-Lite"><i class="fas fa-download"></i> ...</span></a>
                    <a href="https://mmluprox.github.io/" target="_blank"><i class="fas fa-globe"></i> Project</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Thinking Out Loud: Do Reasoning Models Know When They're Right?</span>
                  <span class="pub-authors">Qingcheng Zeng*, <strong>Weihao Xuan*</strong>, Leyang Cui, & Rob Voigt†</span>
                  <span class="pub-venue">In The 2025 Conference on Empirical Methods in Natural Language Processing (EMNLP 2025 Main Conference).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2504.06564" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Geo3DVQA: Evaluating Vision-Language Models for 3D Geospatial Reasoning from Aerial Imagery</span>
                  <span class="pub-authors">Mai Tsujimoto, Junjue Wang, <strong>Weihao Xuan</strong>, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The IEEE/CVF Winter Conference on Applications of Computer Vision (WACV 2026).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2512.07276" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">LR2Depth: Large-Region Aggregation at Low Resolution for Efficient Monocular Depth Estimation</span>
                  <span class="pub-authors">Chao Ning, <strong>Weihao Xuan</strong>, Wanshui Gan, & Naoto Yokoya†</span>
                  <span class="pub-venue">In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025)</span>
                  <div class="pub-links">
                    <a href="https://ieeexplore.ieee.org/document/11246436/" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="#" target="_blank"><i class="fab fa-github"></i> Code</a>
                  </div>
                </div>
              </li>

              <li class="pub-item featured" id="neurips2024-spotlight">
                <div class="pub-content">
                  <span class="pub-title">SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding From Monocular Remote Sensing Imagery</span>
                  <span class="pub-authors">Jian Song, Hongruixuan Chen, <strong>Weihao Xuan</strong>, Junshi Xia, & Naoto Yokoya†</span>
                  <span class="pub-venue">In The Thirty-eight Conference on Neural Information Processing Systems (NeurIPS 2024).</span>
                  <span class="pub-badge"><i class="fas fa-lightbulb"></i> Spotlight Paper [Top 3.1%]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2406.18151" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/JTRNEO/SynRS3D" target="_blank"><i class="fas fa-database"></i> Dataset <span class="hf-stats" data-dataset="JTRNEO/SynRS3D"><i class="fas fa-download"></i> ...</span></a>
                    <a href="https://github.com/JTRNEO/SynRS3D" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="JTRNEO/SynRS3D">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item featured" id="eccv2024-oral">
                <div class="pub-content">
                  <span class="pub-title">CAT-SAM: Conditional Tuning for Few-Shot Adaptation of Segment Anything Model</span>
                  <span class="pub-authors">Aoran Xiao*, <strong>Weihao Xuan*</strong>, Heli Qi, Yun Xing, Ruijie Ren, Xiaoqin Zhang, Ling Shao & Shijian Lu†</span>
                  <span class="pub-venue">In European Conference on Computer Vision (ECCV 2024) (pp. 189-206).</span>
                  <span class="pub-badge"><i class="fas fa-microphone"></i> Oral Paper [Top 2.3%, 200/8585]</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2402.03631" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/weihao1115/cat-sam" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="weihao1115/cat-sam">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds</span>
                  <span class="pub-authors">Aoran Xiao, Jiaxing Huang, <strong>Weihao Xuan</strong>, Ruijie Ren, Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik, Shijian Lu†, & Eric Xing</span>
                  <span class="pub-venue">In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023) (pp. 9382-9392).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2304.00690" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://github.com/xiaoaoran/SemanticSTF" target="_blank"><i class="fab fa-github"></i> Code <span class="github-stats" data-repo="xiaoaoran/SemanticSTF">★ ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">On a Network SIS Model With Opinion Dynamics</span>
                  <span class="pub-authors"><strong>Weihao Xuan</strong>, Ruijie Ren, Philip E. Paré, Mengbin Ye, Sebastian Ruf, & Ji Liu†</span>
                  <span class="pub-venue">IFAC-PapersOnLine, 53(2), 2582-2587.</span>
                  <div class="pub-links">
                    <a href="https://www.sciencedirect.com/science/article/pii/S2405896320305863" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>
            </ol>

            <h3 class="pub-category-title">Journal Papers</h3>
            <ol class="pub-list">
              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">AI for Earthquake Response: Outcomes & Insights from a Global Spaceborne Rapid Mapping Challenge</span>
                  <span class="pub-authors">Patrick Ebel, Mounia El Baz, Junjue Wang, <strong>Weihao Xuan</strong>, Heli Qi, Zhuo Zheng, Naoto Yokoya, Junghwan Park, Jaewan Park, Arthur Elskens, Eléonore Charles, Zachary Foltz, Iacopo Modica, Philippe Bally, Christian Bossung, Marco Chini, Nicolas Longépé, & Gabriele Meoni</span>
                  <span class="pub-venue">IEEE Geoscience and Remote Sensing Magazine, accepted.</span>
                  <div class="pub-links">
                    <a href="#" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">CityVLM: Towards Sustainable Urban Development via Multi-View Coordinated Vision–Language Model</span>
                  <span class="pub-authors">Junjue Wang*, <strong>Weihao Xuan*</strong>, Heli Qi, Zihang Chen, Hongruixuan Chen, Zhuo Zheng, Junshi Xia, Yanfei Zhong, & Naoto Yokoya†</span>
                  <span class="pub-venue">ISPRS Journal of Photogrammetry and Remote Sensing.</span>
                  <div class="pub-links">
                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0924271625004678" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">BRIGHT: A Globally Distributed Multimodal Building Damage Assessment Dataset With Very-High-Resolution for All-Weather Disaster Response</span>
                  <span class="pub-authors">Hongruixuan Chen, Jian Song, Olivier Dietrich, Clifford Broni-Bediako, <strong>Weihao Xuan</strong>, Junjue Wang, Xinlei Shao, Yimin Wei, Junshi Xia, Cuiling Lan, Konrad Schindler, & Naoto Yokoya†</span>
                  <span class="pub-venue">Earth System Science Data (ESSD) (also in ICCV 2025 SEA Workshop, IGARSS 2025).</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2501.06019" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                    <a href="https://huggingface.co/datasets/Kullervo/BRIGHT" target="_blank"><i class="fas fa-database"></i> Dataset <span class="hf-stats" data-dataset="Kullervo/BRIGHT"><i class="fas fa-download"></i> ...</span></a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">Foundation Models for Remote Sensing and Earth Observation: A Survey</span>
                  <span class="pub-authors">Aoran Xiao, <strong>Weihao Xuan</strong>, Junjue Wang, Jiaxing Huang, Dacheng Tao, Shijian Lu†, & Naoto Yokoya†</span>
                  <span class="pub-venue">IEEE Geoscience and Remote Sensing Magazine.</span>
                  <div class="pub-links">
                    <a href="https://arxiv.org/pdf/2410.16602" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>

              <li class="pub-item">
                <div class="pub-content">
                  <span class="pub-title">TSG-Seg: Temporal-selective guidance for semi-supervised semantic segmentation of 3D LiDAR point clouds</span>
                  <span class="pub-authors"><strong>Weihao Xuan</strong>, Heli Qi, & Aoran Xiao</span>
                  <span class="pub-venue">ISPRS Journal of Photogrammetry and Remote Sensing.</span>
                  <div class="pub-links">
                    <a href="https://www.sciencedirect.com/science/article/abs/pii/S0924271624002879" target="_blank"><i class="fas fa-file-pdf"></i> Paper</a>
                  </div>
                </div>
              </li>
            </ol>
          </div>
        </div>
      </div>
    </section>


    <section id="education">
      <div class="container">
        <h2 class="section-title">Education</h2>
        <div class="education-content">
          <div class="education-item">
            <span class="education-university">The University of Tokyo (東京大学)</span>
            <span class="education-degree">Ph.D. Candidate in Complexity Science and Engineering</span>
            <div class="education-details"><a href="https://www.ms.k.u-tokyo.ac.jp/members.html">Machine Learning and Statistical Data Analysis Lab (杉山・横矢・石田研究室)</a>, Advisor: Prof. <a href="https://naotoyokoya.com/">Naoto Yokoya</a></div>
            <div class="education-details"><span class="education-position">Junior Research Associate (JRA)</span> at <a href="https://www.riken.jp/">RIKEN</a> <a href="https://aip.riken.jp/?lang=en">Center for Advanced Intelligence Project</a></div>
          </div>

          <div class="education-item">
            <span class="education-university">Waseda University (早稲田大学)</span>
            <span class="education-degree">M.Eng. in Information</span>
            <div class="education-details"><span class="education-highlight">Okuma Memorial Scholarship (Top Student)</span></div>
          </div>

          <div class="education-item">
            <span class="education-university">University of Leeds</span>
            <span class="education-degree">B.Eng. in Mechanical Engineering</span>
            <div class="education-details"><span class="education-highlight">First-Class Honours</span></div>
          </div>
        </div>
      </div>
    </section>

    <section id="activities">
      <div class="container">
        <h2 class="section-title">Professional Activities</h2>
        <div class="activities-content">
          <div class="activity-section">
            <h3>Reviewer</h3>
            <div class="activity-row">
              <div class="activity-label">Conference</div>
              <div class="activity-content activity-list">
                <em>NeurIPS</em>
                <em>ICLR</em>
                <em>ICML</em>
                <em>CVPR</em>
                <em>ICCV</em>
                <em>ECCV</em>
                <em>AAAI</em>
                <em>ACL Rolling Review</em>
                <em>ICCVW</em>
                <em>ACMMM</em>
                <em>ICRA</em>
                <em>IROS</em>
                <em>ICDL</em>
                <em>SII</em>
                <em>CPHS</em>
              </div>
          </div>

          <div class="activity-section">
            <h3>Organization</h3>
            <div class="activity-row">
              <div class="activity-label">Session Co-Chair</div>
              <div class="activity-content">ER3: System Integration, <em>IEEE/SICE International Symposium on System Integration</em> (SII 2022)</div>
            </div>
          </div>

        </div>
      </div>
    </section>

    <section id="funding">
      <div class="container">
        <h2 class="section-title">Funding and Awards</h2>
        <div class="funding-content">

          <div class="funding-item">
            <span class="funding-title">NVIDIA Academic Grant</span>
            <div class="funding-date">Dec 2024</div>
          </div>

          <div class="funding-item">
            <span class="funding-title">RIKEN Junior Research Associate</span>
            <div class="funding-date">Dec 2023</div>
          </div>

          <div class="funding-item">
            <span class="funding-title">Okuma Memorial Scholarship (Top Student)</span>
            <div class="funding-date">Dec 2022</div>
          </div>

          <div class="funding-item">
            <span class="funding-title">Monbukagakusho Honors Scholarship, JASSO</span>
            <div class="funding-date">Apr 2022</div>
          </div>
        </div>
      </div>
    </section>

    <footer>
      <div class="container">
        <p>Weihao Xuan | Last updated: December 2025</p>
      </div>
    </footer>

    <script>
      // Back to top button functionality
      const backToTopButton = document.getElementById('backToTop');

      window.addEventListener('scroll', () => {
        if (window.scrollY > 300) {
          backToTopButton.classList.add('visible');
        } else {
          backToTopButton.classList.remove('visible');
        }
      });

      // Auto-organize news items and toggle functionality
      const newsContainer = document.getElementById('newsContainer');
      const newsToggle = document.getElementById('newsToggle');
      const newsCollapsible = document.getElementById('newsCollapsible');
      const newsToggleIcon = document.getElementById('newsToggleIcon');
      const newsToggleText = document.getElementById('newsToggleText');

      // Auto-organize news items
      if (newsContainer && newsCollapsible && newsToggle) {
        const newsItems = Array.from(newsContainer.querySelectorAll('p'));
        const maxVisible = 100;

        if (newsItems.length > maxVisible) {
          // Move items beyond the 6th to collapsible section
          const itemsToMove = newsItems.slice(maxVisible);
          itemsToMove.forEach(item => {
            newsCollapsible.appendChild(item);
          });

          // Show the toggle button
          newsToggle.style.display = 'inline-flex';
        }

        // Toggle functionality
        newsToggle.addEventListener('click', () => {
          const isExpanded = newsCollapsible.classList.contains('expanded');

          if (isExpanded) {
            newsCollapsible.classList.remove('expanded');
            newsToggleIcon.className = 'fas fa-chevron-down';
            newsToggleText.textContent = 'Show More';
          } else {
            newsCollapsible.classList.add('expanded');
            newsToggleIcon.className = 'fas fa-chevron-up';
            newsToggleText.textContent = 'Show Less';
          }
        });
      }

      // Smooth scrolling for anchor links
      document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener('click', function (e) {
          e.preventDefault();
          const target = document.querySelector(this.getAttribute('href'));
          if (target) {
            target.scrollIntoView({
              behavior: 'smooth',
              block: 'start'
            });
          }
        });
      });

      // Load GitHub stars count using Shields.io API with GitHub API fallback
      async function loadGitHubStars() {
        const githubElements = document.querySelectorAll('.github-stats[data-repo]');

        for (const element of githubElements) {
          const repo = element.getAttribute('data-repo');
          try {
            // Try Shields.io API first
            const response = await fetch(`https://img.shields.io/github/stars/${repo}.json`);
            const data = await response.json();

            // Check if Shields.io returned valid data
            if (data.value !== undefined && !data.value.includes('Unable to select')) {
              element.innerHTML = `★ ${data.value.toLocaleString()}`;
            } else {
              // Fallback to GitHub API
              try {
                const ghResponse = await fetch(`https://api.github.com/repos/${repo}`);
                const ghData = await ghResponse.json();
                if (ghData.stargazers_count !== undefined) {
                  element.innerHTML = `★ ${ghData.stargazers_count.toLocaleString()}`;
                } else {
                  element.innerHTML = '★';
                }
              } catch (ghError) {
                console.log(`Failed to load stars from GitHub API for ${repo}:`, ghError);
                element.innerHTML = '★';
              }
            }
          } catch (error) {
            console.log(`Failed to load stars for ${repo}:`, error);
            element.innerHTML = '★';
          }
        }
      }

      // Load Hugging Face download stats
      async function loadHuggingFaceStats() {
        const hfElements = document.querySelectorAll('.hf-stats[data-dataset]');

        for (const element of hfElements) {
          const dataset = element.getAttribute('data-dataset');
          try {
            // Try to get dataset info with all-time downloads from HF API
            const response = await fetch(`https://huggingface.co/api/datasets/${dataset}?expand=downloadsAllTime`);
            const data = await response.json();

            // Check for total downloads (all-time) first, then fallback to monthly
            let downloadCount = null;
            let isAllTime = false;

            if (data.downloadsAllTime) {
              downloadCount = data.downloadsAllTime;
              isAllTime = true;
            } else if (data.downloads) {
              downloadCount = data.downloads;
              isAllTime = false;
            }

            if (downloadCount && downloadCount > 0) {
              // Show total downloads or monthly downloads with indicator
              const suffix = isAllTime ? '' : '/mo';
              element.innerHTML = `<i class="fas fa-download"></i> ${downloadCount.toLocaleString()}${suffix}`;
            } else {
              // Fallback: try to get from the main datasets API endpoint
              try {
                const searchResponse = await fetch(`https://huggingface.co/api/datasets?search=${encodeURIComponent(dataset)}&expand=downloadsAllTime&limit=1`);
                const searchData = await searchResponse.json();

                if (searchData && searchData.length > 0) {
                  const item = searchData[0];
                  if (item.downloadsAllTime) {
                    element.innerHTML = `<i class="fas fa-download"></i> ${item.downloadsAllTime.toLocaleString()}`;
                  } else if (item.downloads) {
                    element.innerHTML = `<i class="fas fa-download"></i> ${item.downloads.toLocaleString()}/mo`;
                  } else {
                    element.innerHTML = '<i class="fas fa-download"></i> HF';
                  }
                } else {
                  element.innerHTML = '<i class="fas fa-download"></i> HF';
                }
              } catch (fallbackError) {
                element.innerHTML = '<i class="fas fa-download"></i> HF';
              }
            }
          } catch (error) {
            console.log(`Failed to load stats for ${dataset}:`, error);
            element.innerHTML = '<i class="fas fa-download"></i> HF';
          }
        }
      }

      // Hidden collaborators functionality
      const collaboratorsLink = document.getElementById('collaboratorsLink');
      const hiddenCollaborators = document.getElementById('hiddenCollaborators');


      // Toggle functionality for showing/hiding entire collaborators section
      if (collaboratorsLink && hiddenCollaborators) {
        const originalHTML = 'amazing collaborators <i class="fas fa-users" style="font-size: 0.8em; opacity: 0.6; margin-left: 2px;"></i>';

        collaboratorsLink.addEventListener('click', (e) => {
          e.preventDefault();
          const isVisible = hiddenCollaborators.style.display !== 'none';

          if (isVisible) {
            // Fade out
            hiddenCollaborators.style.opacity = '0';
            setTimeout(() => {
              hiddenCollaborators.style.display = 'none';
            }, 300);
            collaboratorsLink.innerHTML = originalHTML;
          } else {
            // Fade in
            hiddenCollaborators.style.display = 'block';
            setTimeout(() => {
              hiddenCollaborators.style.opacity = '1';
            }, 10);
            collaboratorsLink.innerHTML = 'amazing collaborators <i class="fas fa-users" style="font-size: 0.8em; opacity: 0.6; margin-left: 2px;"></i> <small style="opacity: 0.7;">(click to hide)</small>';
          }
        });
      }

      // Load Hugging Face model stats
      async function loadHuggingFaceModelStats() {
        const hfModelElements = document.querySelectorAll('.hf-stats[data-model]');

        for (const element of hfModelElements) {
          const model = element.getAttribute('data-model');
          try {
            // Try to get model info with all-time downloads from HF API
            const response = await fetch(`https://huggingface.co/api/models/${model}?expand=downloadsAllTime`);
            const data = await response.json();

            // Check for total downloads (all-time) first, then fallback to monthly
            let downloadCount = null;
            let isAllTime = false;

            if (data.downloadsAllTime) {
              downloadCount = data.downloadsAllTime;
              isAllTime = true;
            } else if (data.downloads) {
              downloadCount = data.downloads;
              isAllTime = false;
            }

            if (downloadCount && downloadCount > 0) {
              // Show total downloads or monthly downloads with indicator
              const suffix = isAllTime ? '' : '/mo';
              element.innerHTML = `<i class="fas fa-download"></i> ${downloadCount.toLocaleString()}${suffix}`;
            } else {
              element.innerHTML = '<i class="fas fa-download"></i> HF';
            }
          } catch (error) {
            console.log(`Failed to load stats for model ${model}:`, error);
            element.innerHTML = '<i class="fas fa-download"></i> HF';
          }
        }
      }

      // Load stats when page loads
      window.addEventListener('load', () => {
        loadGitHubStars();
        loadHuggingFaceStats();
        loadHuggingFaceModelStats();
      });
    </script>
  </body>
</html>
